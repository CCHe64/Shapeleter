{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-12-19T14:32:46.448396Z",
     "iopub.status.busy": "2020-12-19T14:32:46.447602Z",
     "iopub.status.idle": "2020-12-19T14:32:51.904418Z",
     "shell.execute_reply": "2020-12-19T14:32:51.905034Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Recommended Versions\\n    numpy==1.22.4\\n    pandas==1.5.3\\n    sklearn==1.5.0\\n    aeon==1.1.0\\n    hypergraphx==1.7.8\\n    torch==2.2.0+cpu\\n    scipy==1.8.1\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import sliding_window_view\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from aeon.transformations.collection.shapelet_based import RandomDilatedShapeletTransform\n",
    "import hypergraphx as hgx\n",
    "import torch \n",
    "import torch.nn as nn \n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F \n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "\n",
    "\"\"\" Recommended Versions\n",
    "    numpy==1.22.4\n",
    "    pandas==1.5.3\n",
    "    sklearn==1.5.0\n",
    "    aeon==1.1.0\n",
    "    hypergraphx==1.7.8\n",
    "    torch==2.2.0+cpu\n",
    "    scipy==1.8.1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Base Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hard Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def voting(testYList, \n",
    "           weightList=None, \n",
    "           p=0):\n",
    "    \"\"\"\n",
    "    Hard voting\n",
    "\n",
    "    \"\"\"\n",
    "    uniqueLabels = np.unique(testYList) \n",
    "    L = len(uniqueLabels)\n",
    "    K, N = len(testYList), len(testYList[0])  \n",
    "    testVY = np.zeros(N) \n",
    "    if weightList is None: \n",
    "        weightList = np.ones(K)\n",
    "   \n",
    "    testWeightArray = np.zeros((L, N)) \n",
    "    for i in range(N): \n",
    "        for j in range(K): \n",
    "            label_ = testYList[j, i]  \n",
    "            index_ = np.arange(L)[uniqueLabels==label_] \n",
    "            testWeightArray[index_, i] += weightList[j] \n",
    "    for i in range(N):\n",
    "        testVY[i] = uniqueLabels[np.argmax(testWeightArray[:, i])]\n",
    "    return testVY.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sereis Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def HT(signal):\n",
    "    \"\"\"\n",
    "    Hilbert transform.\n",
    "\n",
    "    \"\"\"\n",
    "    N = len(signal) \n",
    "    rangeN = np.arange(N).astype(float) + 1.0  \n",
    "    signalH = np.zeros(N)  \n",
    "    for k in range(1, len(signal)+1):\n",
    "        signalH[k-1] = np.sum(signal[0:k-1] / (k-rangeN)[0:k-1]) + np.sum(signal[k:] / (k-rangeN)[k:])\n",
    "    return signalH\n",
    "\n",
    "def series_transform(seriesX, \n",
    "                     mode=\"F\"):  \n",
    "    \"\"\"\n",
    "    Series transformation of time series sets.\n",
    "\n",
    "    \"\"\"\n",
    "    seriesN, seriesD, seriesL = seriesX.shape[0], seriesX.shape[1], seriesX.shape[2]\n",
    "    \n",
    "    if mode == \"F\":  # Differential transform\n",
    "        seriesFX = np.zeros((seriesN, seriesD, seriesL-1))\n",
    "        for i in range(seriesN):\n",
    "            for j in range(seriesD):\n",
    "                seriesFX[i, j, :] = np.diff(seriesX[i, j, :], 1)\n",
    "                seriesFX[i, j, :] = scale(seriesFX[i, j, :])\n",
    "        return seriesFX\n",
    "    \n",
    "    if mode == \"H\":  # Hilbert transform\n",
    "        seriesHX = np.zeros((seriesN, seriesD, seriesL))\n",
    "        for i in range(seriesN): \n",
    "            for j in range(seriesD): \n",
    "                seriesHX[i, j, :] = HT(seriesX[i, j, :])\n",
    "                seriesHX[i, j, :] = scale(seriesHX[i, j, :])\n",
    "        return seriesHX    \n",
    "    \n",
    "    if mode == \"HF\":  # Differential transform after Hilbert transform\n",
    "        seriesHFX = np.zeros((seriesN, seriesD, seriesL-1))\n",
    "        for i in range(seriesN):  \n",
    "            for j in range(seriesD):  \n",
    "                seriesX_ = HT(seriesX[i, j, :]) \n",
    "                seriesHFX[i, j, :] = np.diff(seriesX_, 1)  \n",
    "                seriesHFX[i, j, :] = scale(seriesHFX[i, j, :])\n",
    "        return seriesHFX   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shapelet Hypergraph Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shapelets_to_hypergraph_neighbors(shapelets, \n",
    "                                      neighbor_k=[3, 5], \n",
    "                                      normalize=False):\n",
    "    \"\"\"\n",
    "    Generate hypergraphs using shapelets as nodes.\n",
    "\n",
    "    \"\"\"\n",
    "    shapeletList = np.zeros_like(shapelets)\n",
    "    shapeletList[::] = shapelets[::]\n",
    "    n_shapelet, dim_shapelet, len_shapelet = shapeletList.shape[::]  \n",
    "    \n",
    "    similarity_matrix = np.zeros((n_shapelet, n_shapelet)) \n",
    "    \n",
    "    if normalize:  \n",
    "        for i in range(n_shapelet):\n",
    "            shapeletList[i, 0, :] = scale(shapeletList[i, 0, :])\n",
    "\n",
    "    similarity_matrix = euclidean_distances(shapeletList[:, 0, :])  # Distance matrix\n",
    "    np.fill_diagonal(similarity_matrix, -np.inf)\n",
    "    \n",
    "    HGlist = []\n",
    "    for k in range(len(neighbor_k)): \n",
    "        edges = [] \n",
    "        for i in range(len(similarity_matrix)): \n",
    "            index_ = np.argsort(similarity_matrix[i])[:neighbor_k[k]] \n",
    "            edges.append(index_)  \n",
    "        HGlist.append(edges) \n",
    "        \n",
    "    return HGlist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jaccard Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_jaccard_matrix(edges): \n",
    "    \"\"\"\n",
    "    Calculate the Jaccard similarity matrix for all node pairs in the hypergraph.\n",
    "    \"\"\"\n",
    "    \n",
    "    unique_nodes = np.unique(np.hstack(edges)) \n",
    "    nodes = unique_nodes.tolist() \n",
    "\n",
    "    num_nodes = len(nodes)\n",
    "    num_edges = len(edges)\n",
    "    \n",
    "    node_to_idx = {node: idx for idx, node in enumerate(nodes)}\n",
    "    \n",
    "    H = lil_matrix((num_nodes, num_edges), dtype=int)  # Incidence matrix \n",
    "    \n",
    "    for edge_id, edge in enumerate(edges):\n",
    "        for node in edge:\n",
    "            if node in node_to_idx:  \n",
    "                node_idx = node_to_idx[node]\n",
    "                H[node_idx, edge_id] = 1\n",
    "    \n",
    "    H = H.tocsr() \n",
    "    intersection = H.dot(H.T) \n",
    "    degrees = np.array(H.sum(axis=1)).flatten()\n",
    "\n",
    "    intersection_dense = intersection.toarray() \n",
    "    union = degrees[:, np.newaxis] + degrees[np.newaxis, :] - intersection_dense \n",
    "    J_matrix = np.zeros((num_nodes, num_nodes), dtype=float)\n",
    "    np.divide(intersection_dense, union, where=(union != 0), out=J_matrix)\n",
    "    J_matrix[union == 0] = 0.0\n",
    "    np.fill_diagonal(J_matrix, 1.0)\n",
    "    \n",
    "    return J_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Scale Shapelet Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def neighbor_hypergraph_based_shapelet_selection(shapeletList, \n",
    "                                                 shapeletLabels, \n",
    "                                                 neighbor_k=[3, 5],\n",
    "                                                 save_rate=0.5, \n",
    "                                                 jaccard_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Select shapelets with the same length and dilation based on hypergraphs\n",
    "    \n",
    "    \"\"\"\n",
    "    n_shapelets = len(shapeletLabels) \n",
    "    \n",
    "    neighbor_k = np.array(neighbor_k)\n",
    "    neighbor_k = neighbor_k[neighbor_k<=n_shapelets]  \n",
    "    if len(neighbor_k)==0:\n",
    "        return np.arange(n_shapelets)\n",
    "    \n",
    "    HGlist = shapelets_to_hypergraph_neighbors(shapeletList, neighbor_k=neighbor_k)  # Shapelet hypergraphs\n",
    "    edges = [] \n",
    "    for i in range(len(HGlist)):\n",
    "        for j in range(len(HGlist[i])):\n",
    "            edges.append(HGlist[i][j])\n",
    "    nodes = np.unique(np.hstack(edges))  \n",
    "    \n",
    "    # Remove confusion nodes\n",
    "    purityList = np.zeros(n_shapelets)\n",
    "    degreeList = np.zeros(n_shapelets) \n",
    "    deleteNodeIndexes = [] \n",
    "    for i in range(len(edges)):  \n",
    "        node_index_ = np.array(edges[i])  \n",
    "        subLables_ = shapeletLabels[node_index_] \n",
    "        edge_size_ = len(node_index_)\n",
    "        for j in range(len(node_index_)): \n",
    "            degreeList[node_index_[j]] += 1  \n",
    "            purityList[node_index_[j]] += np.sum(subLables_[j]==subLables_) / edge_size_ \n",
    "    purityList = purityList / degreeList  \n",
    "    \n",
    "    J_matrix = compute_jaccard_matrix(edges)  # Jaccard similarity matrix\n",
    "    J_matrix[J_matrix>=jaccard_threshold] = 1\n",
    "    J_matrix[J_matrix<jaccard_threshold] = 0\n",
    "    np.fill_diagonal(J_matrix, -1.0)\n",
    "\n",
    "    # Shapelet selection\n",
    "    uniqueLables = np.unique(shapeletLabels) \n",
    "    save_shapelet_n = int(np.ceil(save_rate*n_shapelets)) \n",
    "    saved_nodes = np.array([np.argmax(purityList)])  \n",
    "    label_index = np.arange(len(uniqueLables))[uniqueLables==shapeletLabels[saved_nodes]] \n",
    "    label_index += 1\n",
    "    if label_index>=len(uniqueLables):\n",
    "        label_index = 0        \n",
    "    sorted_nodes = np.argsort(purityList)[::-1]\n",
    "    candidate_nodes = sorted_nodes[sorted_nodes!=saved_nodes[0]] \n",
    "    for i in range(save_shapelet_n-1): \n",
    "        similar_nodes_ = np.arange(n_shapelets)[J_matrix[saved_nodes[-1], :]==1]\n",
    "        J_matrix[:, similar_nodes_] = -1 \n",
    "        J_matrix[similar_nodes_, :] = -1\n",
    "        J_matrix[saved_nodes[-1], :] = -1  \n",
    "        J_matrix[:, saved_nodes[-1]] = -1\n",
    "        candidate_nodes = candidate_nodes[~np.isin(candidate_nodes, similar_nodes_)]\n",
    "        candidate_labels = shapeletLabels[candidate_nodes]\n",
    "\n",
    "        if len(candidate_nodes) <= 0: \n",
    "            break    \n",
    "        curr_label_ = uniqueLables[label_index]  \n",
    "        if np.sum(candidate_labels==curr_label_)==0: \n",
    "            curr_label_0 = curr_label_ \n",
    "            label_index += 1\n",
    "            if label_index>=len(uniqueLables):  \n",
    "                label_index = 0\n",
    "            curr_label_ = uniqueLables[label_index]  \n",
    "            uniqueLables = uniqueLables[uniqueLables!=curr_label_0]  \n",
    "        sub_candidate_nodes = candidate_nodes[candidate_labels==curr_label_]  \n",
    "        if len(sub_candidate_nodes)==0:\n",
    "            break\n",
    "        top_node_ = sub_candidate_nodes[np.argmax(purityList[sub_candidate_nodes])]\n",
    "\n",
    "        label_index += 1 \n",
    "        if label_index>=len(uniqueLables): \n",
    "            label_index = 0          \n",
    "\n",
    "        saved_nodes = np.hstack((saved_nodes, [top_node_]))  \n",
    "        candidate_nodes = candidate_nodes[candidate_nodes!=saved_nodes[-1]]  \n",
    "    return saved_nodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypergraph-based Shapelet Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hypergraph_shapelet_selection(trainSignalX, trainY, \n",
    "                                  max_shapelets=20000, \n",
    "                                  per_shapelets=2500, \n",
    "                                  neighbor_k=[3, 5],  \n",
    "                                  jaccard_threshold=0.9,  \n",
    "                                  seed=0):\n",
    "    \"\"\"\n",
    "    Select Shapelets at Different Scales Based on Hypergraphs\n",
    "    \"\"\"\n",
    "    # Shapelets initialization\n",
    "    ShapeletTransform = RandomDilatedShapeletTransform(max_shapelets=max_shapelets, random_state=seed, alpha_similarity=0.0, proba_normalization=0.8)\n",
    "    ShapeletTransform.fit(trainSignalX, trainY)\n",
    "    shapelets = np.zeros_like(ShapeletTransform.shapelets_[0])\n",
    "    shapelets[::] = ShapeletTransform.shapelets_[0][::]\n",
    "    shapeletLabels = np.zeros_like(ShapeletTransform.shapelets_[8])\n",
    "    shapeletLabels[::] = ShapeletTransform.shapelets_[8][::]\n",
    "    shapeletDilas = np.zeros_like(ShapeletTransform.shapelets_[3])\n",
    "    shapeletDilas[::] = ShapeletTransform.shapelets_[3][::]\n",
    "    shapeletNormal = np.zeros_like(ShapeletTransform.shapelets_[5]) \n",
    "    shapeletNormal[::] = ShapeletTransform.shapelets_[5][::]\n",
    "\n",
    "    uniqueDilas = np.unique(shapeletDilas) \n",
    "    selectedShapeletIndexes = []\n",
    "    save_rate = per_shapelets / len(shapelets)\n",
    "\n",
    "    for j in range(len(uniqueDilas)):\n",
    "        # Select standardized Shapelets\n",
    "        shapeletIndexD_True = np.arange(len(shapeletLabels))[np.logical_and(shapeletDilas==uniqueDilas[j], shapeletNormal==1)] \n",
    "        saveRealIndexesD_True = neighbor_hypergraph_based_shapelet_selection(shapelets[shapeletIndexD_True], shapeletLabels[shapeletIndexD_True], \n",
    "                                                                         neighbor_k=neighbor_k, \n",
    "                                                                         save_rate=save_rate,\n",
    "                                                                         jaccard_threshold=jaccard_threshold,\n",
    "                                                                         )  \n",
    "        if len(saveRealIndexesD_True)>0:\n",
    "            selectedShapeletIndexes.append(shapeletIndexD_True[saveRealIndexesD_True])  \n",
    "\n",
    "        # Select no standardized Shapelets\n",
    "        shapeletIndexD_False = np.arange(len(shapeletLabels))[np.logical_and(shapeletDilas==uniqueDilas[j], shapeletNormal==0)] \n",
    "        saveRealIndexesD_False = neighbor_hypergraph_based_shapelet_selection(shapelets[shapeletIndexD_False], shapeletLabels[shapeletIndexD_False], \n",
    "                                                                         neighbor_k=neighbor_k, \n",
    "                                                                         save_rate=save_rate,\n",
    "                                                                         jaccard_threshold=jaccard_threshold,\n",
    "                                                                         ) \n",
    "        if len(saveRealIndexesD_False)>0:  \n",
    "            selectedShapeletIndexes.append(shapeletIndexD_False[saveRealIndexesD_False]) \n",
    "    selectedShapeletIndexes = np.hstack(selectedShapeletIndexes) \n",
    "\n",
    "    # Update shapelets parameters\n",
    "    ShapeletTransform.shapelets_ = (ShapeletTransform.shapelets_[0][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[1][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[2][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[3][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[4][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[5][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[6][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[7][selectedShapeletIndexes],\n",
    "                            ShapeletTransform.shapelets_[8][selectedShapeletIndexes])    \n",
    "    \n",
    "    return ShapeletTransform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  PSO Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def rdst_so_scaling(featureX, \n",
    "                    series_length, \n",
    "                    shapelet_lengths, \n",
    "                    shapelet_dilations):\n",
    "    \"\"\"\n",
    "    Shapelet SO features transform to PSO features\n",
    "    \n",
    "    \"\"\"\n",
    "    for i in range(featureX.shape[1]): \n",
    "        length_ = shapelet_lengths[i] \n",
    "        dilation_ = shapelet_dilations[i] \n",
    "        featureX[:, i] = featureX[:, i] / (series_length - (length_-1)*dilation_)\n",
    "    \n",
    "    return featureX"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ordinal Positions Conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def assign_order(positionsABS):\n",
    "    \"\"\"\n",
    "    Convert the absolute position of Shapelets to ordinal positions\n",
    "    \"\"\"\n",
    "    n_shapelets = len(positionsABS) \n",
    "    positionsORD = np.zeros_like(positionsABS)  \n",
    "    \n",
    "    unique_positions = np.unique(positionsABS)\n",
    "    frontN = 0  \n",
    "    for i in range(len(unique_positions)):  \n",
    "        currentN = np.sum(positionsABS==unique_positions[i])\n",
    "        positionsORD[positionsABS==unique_positions[i]] = frontN + (currentN-1) / 2  \n",
    "        frontN = frontN + currentN \n",
    "        \n",
    "    return positionsORD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimization of Shapelet Position Encoding Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShapeletDualPositionEncoding(nn.Module):\n",
    "    \"\"\"\n",
    "    Optimize absolute embedding parameter Ka and ordinal embedding parameter Ko through backpropagation\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_coding=2, n_shapelet=10, len_map=100, Ka0=1.5, Ko0=1.5, K_min=1.0, K_max=10.0):\n",
    "        super().__init__() \n",
    "        self.dim_coding = dim_coding \n",
    "        self.n_shapelet = n_shapelet \n",
    "        self.len_map = len_map \n",
    "        self.log_Ka = nn.Parameter(torch.tensor(np.log(Ka0)))  \n",
    "        self.log_Ko = nn.Parameter(torch.tensor(np.log(Ko0))) \n",
    "        self.log_K_min = np.log(K_min)  \n",
    "        self.log_K_max = np.log(K_max) \n",
    "    \n",
    "    def shapelet_absolute_position_encoding(self, pos):\n",
    "        log_Ka = torch.clamp(self.log_Ka, self.log_K_min, self.log_K_max)  \n",
    "        base = torch.exp(log_Ka) * self.len_map / (2*np.pi)\n",
    "        pe = torch.zeros(pos.size(0), self.n_shapelet * self.dim_coding) \n",
    "        pe[:, :self.n_shapelet] = torch.sin(pos / base)  \n",
    "        pe[:, self.n_shapelet:] = torch.cos(pos / base)\n",
    "        return pe  \n",
    "    \n",
    "    def shapelet_ordinal_position_encoding(self, order):\n",
    "        log_Ko = torch.clamp(self.log_Ko, self.log_K_min, self.log_K_max)  \n",
    "        base = torch.exp(log_Ko) * self.n_shapelet / (2*np.pi) \n",
    "        pe = torch.zeros(order.size(0), self.n_shapelet * self.dim_coding)  \n",
    "        pe[:, :self.n_shapelet] = torch.sin(order / base) \n",
    "        pe[:, self.n_shapelet:] = torch.cos(order / base) \n",
    "        return pe \n",
    "    \n",
    "    def forward(self, featureX, positionX, orderX):\n",
    "        sape = self.shapelet_absolute_position_encoding(positionX) \n",
    "        sope = self.shapelet_ordinal_position_encoding(orderX) \n",
    "        featureX_A = featureX + sape  \n",
    "        featureX_O = featureX + sope  \n",
    "        return featureX_A, featureX_O \n",
    "\n",
    "# Similarity between feature vector sets\n",
    "def cosine_similarity_squared_loss(featureX0, featureX1, eps=1e-8):\n",
    "    cos_sim = F.cosine_similarity(featureX0, featureX1, dim=0, eps=eps) \n",
    "    cos_sim_sq = cos_sim ** 2  \n",
    "    return cos_sim_sq.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def position_encoding_k_optimizer(featureX, \n",
    "                                  positionX, \n",
    "                                  orderX, \n",
    "                                  dim_coding=2, \n",
    "                                  len_map=100,\n",
    "                                  Ka0=8.0, \n",
    "                                  Ko0=8.0, \n",
    "                                  K_min=1.0, \n",
    "                                  K_max=15.0, \n",
    "                                  lr=0.002, \n",
    "                                  max_inter=200):\n",
    "    \"\"\"\n",
    "    Optimization of location encoding parameters based on backpropagation\n",
    "    \"\"\"\n",
    "    n_sample = positionX.size(0) \n",
    "    n_shapelet = positionX.size(1) \n",
    "\n",
    "    model = ShapeletDualPositionEncoding(dim_coding=dim_coding, n_shapelet=n_shapelet, len_map=len_map,\n",
    "                                         Ka0=Ka0, Ko0=Ko0, K_max=K_max, K_min=K_min)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    objectList = np.zeros(max_inter)  \n",
    "    kaList = np.zeros(max_inter)  \n",
    "    koList = np.zeros(max_inter)  \n",
    "    objectList[::] = np.nan\n",
    "    kaList[::] = np.nan\n",
    "    koList[::] = np.nan\n",
    "\n",
    "    for epoch in range(max_inter):\n",
    "        optimizer.zero_grad() \n",
    "        featureX_A, featureX_O = model(featureX, positionX, orderX)  \n",
    "        \n",
    "        cos_sim_sq_RA = cosine_similarity_squared_loss(featureX, featureX_A) \n",
    "        cos_sim_sq_RO = cosine_similarity_squared_loss(featureX, featureX_O)  \n",
    "        cos_sim_sq_AO = cosine_similarity_squared_loss(featureX_A, featureX_O)  \n",
    "        cos_loss = -(cos_sim_sq_RA + cos_sim_sq_RO + cos_sim_sq_AO) / 3 \n",
    "\n",
    "        cos_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            Ka = torch.exp(model.log_Ka).item() \n",
    "            Ko = torch.exp(model.log_Ko).item()  \n",
    "            if Ka < K_min or Ka > K_max or Ko < K_min or Ko > K_max:\n",
    "                break\n",
    "            objectList[epoch] = cos_loss.item()\n",
    "            kaList[epoch] = Ka\n",
    "            koList[epoch] = Ko\n",
    "  \n",
    "    return kaList, koList, objectList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Dual  Shapelet  Position  Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEmbedding():\n",
    "    \"\"\"\n",
    "    Embedding absolute and ordinal positions into the original Shapelet feature vector set\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        shapeletDilations=None,\n",
    "        seriesL=None, \n",
    "        shapeletL=None, \n",
    "        learn_rate=0.002, \n",
    "        max_inter=200,                          \n",
    "        baseKa0=8.0, \n",
    "        baseKr0=8.0,                          \n",
    "        K_min=1.0, \n",
    "        K_max=15.0\n",
    "    ):\n",
    "        self.shapeletDilations = shapeletDilations\n",
    "        self.seriesL = seriesL\n",
    "        self.shapeletL = shapeletL\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_inter = max_inter\n",
    "        self.baseKa0 = baseKa0\n",
    "        self.baseKr0 = baseKr0\n",
    "        self.K_min = K_min\n",
    "        self.K_max = K_max\n",
    "        \n",
    "    def fit_transform(self, trainX0, trainX1, trainX_arg): \n",
    "        trainX0_ABS, trainX1_ABS = np.zeros_like(trainX0), np.zeros_like(trainX1) \n",
    "        trainX0_ABS[::], trainX1_ABS[::] = trainX0[::], trainX1[::]\n",
    "        trainX0_REL, trainX1_REL = np.zeros_like(trainX0), np.zeros_like(trainX1)  \n",
    "        trainX0_REL[::], trainX1_REL[::] = trainX0[::], trainX1[::]\n",
    "\n",
    "        self.uniqueDilations = np.unique(self.shapeletDilations)\n",
    "        numberPerDilations = np.zeros_like(self.uniqueDilations)  \n",
    "        for i in range(len(self.uniqueDilations)):  \n",
    "            numberPerDilations[i] = np.sum(self.shapeletDilations==self.uniqueDilations[i])\n",
    "\n",
    "        self.baseABS_List = np.zeros(len(self.uniqueDilations))\n",
    "        self.baseREL_List = np.zeros(len(self.uniqueDilations))  \n",
    "        \n",
    "\n",
    "        for d in range(len(self.uniqueDilations)):  # Optimizing the position encoding parameters for each dilation\n",
    "            dilation_ = self.uniqueDilations[d] \n",
    "            len_map_ = self.seriesL-(self.shapeletL-1)*dilation_ \n",
    "            shapeletSubN_ = numberPerDilations[d]  \n",
    "            subIndex_ = np.arange(len(self.shapeletDilations))[self.shapeletDilations==dilation_] \n",
    "\n",
    "            trainXd_ = torch.from_numpy(np.hstack((trainX0[:, subIndex_], trainX1[:, subIndex_])))  \n",
    "            positionX_ = torch.from_numpy(trainX_arg[:, subIndex_]) \n",
    "            orderX_ = torch.zeros_like(positionX_)  \n",
    "            for i in range(positionX_.size(0)): \n",
    "                orderX_[i, :] = torch.from_numpy(assign_order(positionX_[i, :].numpy()))\n",
    "\n",
    "            kaList, krList, objectList= position_encoding_k_optimizer(trainXd_, positionX_, orderX_,\n",
    "                                        dim_coding=2, len_map=len_map_,\n",
    "                                        Ka0=self.baseKa0, Ko0=self.baseKr0, K_min=self.K_min, K_max=self.K_max, \n",
    "                                        lr=self.learn_rate, max_inter=self.max_inter)\n",
    "            baseKa = kaList[~np.isnan(objectList)][-1] \n",
    "            baseKr = krList[~np.isnan(objectList)][-1] \n",
    "\n",
    "            baseABS = baseKa * len_map_ / (2*np.pi)  \n",
    "            baseREL = baseKr * shapeletSubN_ / (2*np.pi) \n",
    "            self.baseABS_List[d] = baseABS\n",
    "            self.baseREL_List[d] = baseREL\n",
    "            \n",
    "        for d in range(len(self.uniqueDilations)):  # Embedding the positions for each dilation separately\n",
    "            dilation_ = self.uniqueDilations[d]  \n",
    "            subIndex_ = np.arange(len(self.shapeletDilations))[self.shapeletDilations==dilation_] \n",
    "            baseABS = self.baseABS_List[d]\n",
    "            baseREL = self.baseREL_List[d]\n",
    "\n",
    "            for i in range(len(trainX_arg)):  # Absolute\n",
    "                positionsABS = trainX_arg[i, subIndex_]  \n",
    "                trainX0_ABS[i, subIndex_] += np.sin(positionsABS / baseABS)  \n",
    "                trainX1_ABS[i, subIndex_] += np.cos(positionsABS / baseABS) \n",
    "\n",
    "            for i in range(len(trainX_arg)):  # Ordinal\n",
    "                positionsABS = trainX_arg[i, subIndex_] \n",
    "                positionsREL = assign_order(positionsABS) \n",
    "                trainX0_REL[i, subIndex_] += np.sin(positionsREL / baseREL) \n",
    "                trainX1_REL[i, subIndex_] += np.cos(positionsREL / baseREL) \n",
    "        return trainX0_ABS, trainX1_ABS, trainX0_REL, trainX1_REL\n",
    "                \n",
    "    def transform(self, testX0, testX1, testX_arg): \n",
    "        testX0_ABS, testX1_ABS = np.zeros_like(testX0), np.zeros_like(testX1) \n",
    "        testX0_ABS[::], testX1_ABS[::] = testX0[::], testX1[::]\n",
    "        testX0_REL, testX1_REL = np.zeros_like(testX0), np.zeros_like(testX1)\n",
    "        testX0_REL[::], testX1_REL[::] = testX0[::], testX1[::]\n",
    "        \n",
    "        for d in range(len(self.uniqueDilations)):  # Embedding the positions for each dilation separately\n",
    "            dilation_ = self.uniqueDilations[d]  \n",
    "            subIndex_ = np.arange(len(self.shapeletDilations))[self.shapeletDilations==dilation_] \n",
    "            baseABS = self.baseABS_List[d]\n",
    "            baseREL = self.baseREL_List[d]\n",
    "\n",
    "            for i in range(len(testX_arg)):\n",
    "                positionsABS = testX_arg[i, subIndex_] \n",
    "                testX0_ABS[i, subIndex_] += np.sin(positionsABS / baseABS) \n",
    "                testX1_ABS[i, subIndex_] += np.cos(positionsABS / baseABS) \n",
    "\n",
    "            for i in range(len(testX_arg)):\n",
    "                positionsABS = testX_arg[i, subIndex_]\n",
    "                positionsREL = assign_order(positionsABS)  \n",
    "                testX0_REL[i, subIndex_] += np.sin(positionsREL / baseREL) \n",
    "                testX1_REL[i, subIndex_] += np.cos(positionsREL / baseREL) \n",
    "        return testX0_ABS, testX1_ABS, testX0_REL, testX1_REL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiview Feature Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinationFusion():\n",
    "    \"\"\"\n",
    "    Paired view feature vector fusion\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self\n",
    "    ):\n",
    "        ()\n",
    "    def fit_transform(self, trainX_, trainX_ABS, trainX_ORD, trainY): \n",
    "        trainX_O_A, trainX_O_R, trainX_A_R = np.zeros_like(trainX_), np.zeros_like(trainX_), np.zeros_like(trainX_)\n",
    "\n",
    "        self.ldaList_O_A = [] \n",
    "        self.ldaList_O_R = [] \n",
    "        self.ldaList_A_R = [] \n",
    "        self.ldaFalgs_O_A = np.zeros(trainX_.shape[1])\n",
    "        self.ldaFalgs_O_R = np.zeros(trainX_.shape[1])\n",
    "        self.ldaFalgs_A_R = np.zeros(trainX_.shape[1])\n",
    "        for i in range(trainX_.shape[1]):\n",
    "            # Fusion of original and absolute embedded features\n",
    "            lda = LinearDiscriminantAnalysis(n_components=1) \n",
    "            try:\n",
    "                lda.fit(np.hstack((trainX_[:, i:i+1], trainX_ABS[:, i:i+1])), trainY) \n",
    "                trainX_F = lda.transform(np.hstack((trainX_[:, i:i+1], trainX_ABS[:, i:i+1]))) \n",
    "                trainX_O_A[:, i:i+1] = trainX_F[::]\n",
    "                self.ldaFalgs_O_A[i] = 1  \n",
    "            except:  \n",
    "                trainX_F = np.mean(np.hstack((trainX_[:, i:i+1], trainX_ABS[:, i:i+1])), axis=1, keepdims=True) \n",
    "                trainX_O_A[:, i:i+1] = trainX_F[::]\n",
    "            self.ldaList_O_A.append(lda)  \n",
    "            \n",
    "            # Fusion of original and ordinal embedded features\n",
    "            lda = LinearDiscriminantAnalysis(n_components=1) \n",
    "            try:\n",
    "                lda.fit(np.hstack((trainX_[:, i:i+1], trainX_ORD[:, i:i+1])), trainY) \n",
    "                trainX_F = lda.transform(np.hstack((trainX_[:, i:i+1], trainX_ORD[:, i:i+1])))  \n",
    "                trainX_O_R[:, i:i+1] = trainX_F[::]\n",
    "                self.ldaFalgs_O_R[i] = 1 \n",
    "            except: \n",
    "                trainX_F = np.mean(np.hstack((trainX_[:, i:i+1], trainX_ORD[:, i:i+1])), axis=1, keepdims=True)  \n",
    "                trainX_O_R[:, i:i+1] = trainX_F[::]\n",
    "            self.ldaList_O_R.append(lda)  \n",
    "\n",
    "            # Fusion of absolute and ordinal embedded features\n",
    "            lda = LinearDiscriminantAnalysis(n_components=1)  \n",
    "            try:\n",
    "                lda.fit(np.hstack((trainX_ABS[:, i:i+1], trainX_ORD[:, i:i+1])), trainY) \n",
    "                trainX_F = lda.transform(np.hstack((trainX_ABS[:, i:i+1], trainX_ORD[:, i:i+1])))  \n",
    "                trainX_A_R[:, i:i+1] = trainX_F[::]\n",
    "                self.ldaFalgs_A_R[i] = 1 \n",
    "            except:  \n",
    "                trainX_F = np.mean(np.hstack((trainX_ABS[:, i:i+1], trainX_ORD[:, i:i+1])), axis=1, keepdims=True) \n",
    "                trainX_A_R[:, i:i+1] = trainX_F[::]\n",
    "            self.ldaList_A_R.append(lda)  \n",
    "\n",
    "        self.threshold = 20 \n",
    "        trainX_O_A[trainX_O_A>self.threshold] = self.threshold\n",
    "        trainX_O_R[trainX_O_R>self.threshold] = self.threshold\n",
    "        trainX_A_R[trainX_A_R>self.threshold] = self.threshold\n",
    "        \n",
    "        # Feature standardization\n",
    "        self.scaler_O_A = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scaler_O_A.fit(trainX_O_A)\n",
    "        trainX_O_A = self.scaler_O_A.transform(trainX_O_A)\n",
    "        self.scaler_O_R = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scaler_O_R.fit(trainX_O_R)\n",
    "        trainX_O_R = self.scaler_O_R.transform(trainX_O_R)\n",
    "        self.scaler_A_R = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scaler_A_R.fit(trainX_A_R)\n",
    "        trainX_A_R = self.scaler_A_R.transform(trainX_A_R)\n",
    "        \n",
    "        return trainX_O_A, trainX_O_R, trainX_A_R\n",
    "\n",
    "    def transform(self, testX_, testX_ABS, testX_ORD): \n",
    "        testX_O_A, testX_O_R, testX_A_R = np.zeros_like(testX_), np.zeros_like(testX_), np.zeros_like(testX_) \n",
    "\n",
    "        for i in range(testX_.shape[1]):\n",
    "            # Fusion of original and absolute embedded features\n",
    "            lda = self.ldaList_O_A[i]\n",
    "            if self.ldaFalgs_O_A[i]==1:         \n",
    "                testX_F = lda.transform(np.hstack((testX_[:, i:i+1], testX_ABS[:, i:i+1]))) \n",
    "                testX_O_A[:, i:i+1] = testX_F[::]\n",
    "            else:\n",
    "                testX_F = np.mean(np.hstack((testX_[:, i:i+1], testX_ABS[:, i:i+1])), axis=1, keepdims=True)  \n",
    "                testX_O_A[:, i:i+1] = testX_F[::]\n",
    "\n",
    "            # Fusion of original and ordinal embedded features\n",
    "            lda = self.ldaList_O_R[i]\n",
    "            if self.ldaFalgs_O_R[i]==1:                \n",
    "                testX_F = lda.transform(np.hstack((testX_[:, i:i+1], testX_ORD[:, i:i+1])))  \n",
    "                testX_O_R[:, i:i+1] = testX_F[::]\n",
    "            else:\n",
    "                testX_F = np.mean(np.hstack((testX_[:, i:i+1], testX_ORD[:, i:i+1])), axis=1, keepdims=True) \n",
    "                testX_O_R[:, i:i+1] = testX_F[::]\n",
    "\n",
    "            # Fusion of absolute and ordinal embedded features\n",
    "            lda = self.ldaList_A_R[i]\n",
    "            if self.ldaFalgs_A_R[i]==1: \n",
    "                testX_F = lda.transform(np.hstack((testX_ABS[:, i:i+1], testX_ORD[:, i:i+1])))\n",
    "                testX_A_R[:, i:i+1] = testX_F[::]\n",
    "            else:\n",
    "                testX_F = np.mean(np.hstack((testX_ABS[:, i:i+1], testX_ORD[:, i:i+1])), axis=1, keepdims=True) \n",
    "                testX_A_R[:, i:i+1] = testX_F[::]\n",
    "\n",
    "        testX_O_A[testX_O_A>self.threshold] = self.threshold\n",
    "        testX_O_R[testX_O_R>self.threshold] = self.threshold\n",
    "        testX_A_R[testX_A_R>self.threshold] = self.threshold\n",
    "        \n",
    "        # Feature standardization\n",
    "        testX_O_A = self.scaler_O_A.transform(testX_O_A)\n",
    "        testX_O_R = self.scaler_O_R.transform(testX_O_R)\n",
    "        testX_A_R = self.scaler_A_R.transform(testX_A_R)\n",
    "        \n",
    "        return testX_O_A, testX_O_R, testX_A_R\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiview Feature Extraction and Fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiviewFusion():\n",
    "    \"\"\"\n",
    "    Multiview shapelet feature extraction and fusion\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        ShapeletTransform=None,\n",
    "        baseKa0=8.0, \n",
    "        baseKr0=8.0, \n",
    "        K_min=1.0, \n",
    "        K_max=15.0,\n",
    "        learn_rate=0.002, \n",
    "        max_inter=200,\n",
    "        seed=0\n",
    "    ):\n",
    "        self.ShapeletTransform = ShapeletTransform\n",
    "        self.baseKa0 = baseKa0\n",
    "        self.baseKr0 = baseKr0\n",
    "        self.K_min = K_min\n",
    "        self.K_max = K_max\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_inter = max_inter\n",
    "        self.seed = seed\n",
    "        \n",
    "    def fit_transform(self, trainSignalX, trainY): \n",
    "        trainX = self.ShapeletTransform.transform(trainSignalX)  # Initial feature extraction\n",
    "        trainX[np.isnan(trainX)] = 0\n",
    "        trainX[np.isinf(trainX)] = 0\n",
    "        \n",
    "        self.seriesL = trainSignalX.shape[2] \n",
    "        shapeletLengths = self.ShapeletTransform.shapelets_[2]  \n",
    "        shapeletL = shapeletLengths[0]  \n",
    "        shapeletDilations = self.ShapeletTransform.shapelets_[3] \n",
    "        \n",
    "        trainX[:, 2::3] = rdst_so_scaling(trainX[:, 2::3], self.seriesL, shapeletLengths, shapeletDilations)[::] \n",
    "        trainX_min, trainX_arg, trainX_soo = trainX[:, 0::3], trainX[:, 1::3], trainX[:, 2::3]\n",
    "        \n",
    "        # Feature standardization\n",
    "        self.scalerMIN = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scalerMIN.fit(trainX_min)\n",
    "        trainX_min = self.scalerMIN.transform(trainX_min)\n",
    "\n",
    "        self.scalerSOO = MinMaxScaler(feature_range=(-1, 1))\n",
    "        self.scalerSOO.fit(trainX_soo)\n",
    "        trainX_soo = self.scalerSOO.transform(trainX_soo)\n",
    "     \n",
    "        # Dual shapelet position encoding\n",
    "        self.positional_embedding = PositionalEmbedding(shapeletDilations=shapeletDilations, \n",
    "                                                        seriesL=self.seriesL, \n",
    "                                                        shapeletL=shapeletL, \n",
    "                                                        learn_rate=self.learn_rate,\n",
    "                                                        max_inter=self.max_inter,\n",
    "                                                        baseKa0=self.baseKa0, \n",
    "                                                        baseKr0=self.baseKr0,\n",
    "                                                        K_min=self.K_min, \n",
    "                                                        K_max=self.K_max)\n",
    "\n",
    "        trainX_minABS, trainX_sooABS, trainX_minREL, trainX_sooREL = self.positional_embedding.fit_transform(trainX_min, trainX_soo, trainX_arg)\n",
    "\n",
    "        # MIN feature fusion\n",
    "        self.combination_fusion_min = CombinationFusion()\n",
    "        trainX_minO_A, trainX_minO_R, trainX_minA_R = self.combination_fusion_min.fit_transform(trainX_min, trainX_minABS, trainX_minREL, trainY)\n",
    "        # PSO feature fusion\n",
    "        self.combination_fusion_soo = CombinationFusion()\n",
    "        trainX_sooO_A, trainX_sooO_R, trainX_sooA_R = self.combination_fusion_soo.fit_transform(trainX_soo, trainX_sooABS, trainX_sooREL, trainY)\n",
    "        \n",
    "        return [trainX_min, trainX_soo, \n",
    "                trainX_minO_A, trainX_minO_R, trainX_minA_R,\n",
    "                trainX_sooO_A, trainX_sooO_R, trainX_sooA_R]\n",
    "        \n",
    "    def transform(self, testSignalX):   \n",
    "        testX = self.ShapeletTransform.transform(testSignalX)  # Initial feature extraction\n",
    "        testX[np.isnan(testX)] = 0\n",
    "        testX[np.isinf(testX)] = 0\n",
    "        \n",
    "        shapeletLengths = self.ShapeletTransform.shapelets_[2]  \n",
    "        shapeletDilations = self.ShapeletTransform.shapelets_[3] \n",
    "                \n",
    "        testX[:, 2::3] = rdst_so_scaling(testX[:, 2::3], self.seriesL, shapeletLengths, shapeletDilations)[::]  \n",
    "        testX_min, testX_arg, testX_soo = testX[:, 0::3], testX[:, 1::3], testX[:, 2::3]\n",
    "        \n",
    "        # Feature standardization\n",
    "        testX_min = self.scalerMIN.transform(testX_min)\n",
    "        testX_soo = self.scalerSOO.transform(testX_soo)\n",
    "        \n",
    "        # Dual shapelet position encoding\n",
    "        testX_minABS, testX_sooABS, testX_minREL, testX_sooREL = self.positional_embedding.transform(testX_min, testX_soo, testX_arg)\n",
    "        \n",
    "        # MIN feature fusion\n",
    "        testX_minO_A, testX_minO_R, testX_minA_R = self.combination_fusion_min.transform(testX_min, testX_minABS, testX_minREL)\n",
    "        # PSO feature fusion\n",
    "        testX_sooO_A, testX_sooO_R, testX_sooA_R = self.combination_fusion_soo.transform(testX_soo, testX_sooABS, testX_sooREL)\n",
    "\n",
    "        return [testX_min, testX_soo, \n",
    "                testX_minO_A, testX_minO_R, testX_minA_R, \n",
    "                testX_sooO_A, testX_sooO_R, testX_sooA_R]      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Shapeleter Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" ShapeleterClassifier\n",
    "    A novel shapelet-based TSC algorithm Shapelet Enhancer (Shapeleter).\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "__maintainer__ = [\"Changchun He\"]\n",
    "__all__ = [\"ShapeleterClassifier\"]\n",
    "\n",
    "class ShapeleterClassifier():\n",
    "    \"\"\"\n",
    "    Shapelet Enhancer (Shapeleter) for Time Series Classification.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    max_shapelets : int, default=20000\n",
    "        The number of shapelet candidates per series representation\n",
    "    per_shapelets : int, default=2500\n",
    "        The number of selected shapelets per series representation\n",
    "    neighbor_k : list, default=[3, 5]\n",
    "        The sizes of shapelet hyperedge\n",
    "    jaccard_threshold : float, default=0.9\n",
    "        The Jaccard similarity threshold\n",
    "    baseKa0 : float, default=8.0\n",
    "        The frequency factor for SAPE\n",
    "    baseKr0 : float, default=8.0\n",
    "        The frequency factor for SOPE\n",
    "    K_max : float, default=15.0\n",
    "        The maximum of frequency factor\n",
    "    K_min : float, default=1.0\n",
    "        The minimum of frequency factor\n",
    "    learn_rate : float, default=0.002\n",
    "        The learning rate\n",
    "    max_inter : int, default=200\n",
    "        The maximum of iterations for positional encoding optimization\n",
    "    random_state: int, default=None\n",
    "        Controls randomness\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        max_shapelets=20000,  \n",
    "        per_shapelets=2500,\n",
    "        neighbor_k=[3, 5],  \n",
    "        jaccard_threshold=0.9,  \n",
    "        baseKa0=8.0,\n",
    "        baseKr0=8.0, \n",
    "        K_max=15.0, \n",
    "        K_min=1.0,\n",
    "        learn_rate=0.002,  \n",
    "        max_inter=200, \n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.max_shapelets = max_shapelets\n",
    "        self.per_shapelets = per_shapelets\n",
    "        self.neighbor_k = neighbor_k\n",
    "        self.jaccard_threshold = jaccard_threshold\n",
    "        self.baseKa0 = baseKa0\n",
    "        self.baseKr0 = baseKr0\n",
    "        self.K_max = K_max\n",
    "        self.K_min = K_min\n",
    "        self.learn_rate = learn_rate\n",
    "        self.max_inter = max_inter\n",
    "        self.random_state = random_state\n",
    "        \n",
    "    def fit(self, trainSignalX, trainY): \n",
    "        \"\"\"Fit a pipeline on cases (trainSignalX, trainY), where trainY is the target variable.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        trainSignalX : 3D np.ndarray of shape = [n_cases, n_channels, n_timepoints]\n",
    "            The training data.\n",
    "        trainY : array-like, shape = [n_cases]\n",
    "            The class labels. Each type of label is int.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        self :\n",
    "            Reference to self.\n",
    "        \"\"\"\n",
    "        # series transform\n",
    "        trainSignalT0X = series_transform(trainSignalX, mode=\"F\")\n",
    "        trainSignalT1X = series_transform(trainSignalX, mode=\"H\")\n",
    "        trainSignalT2X = series_transform(trainSignalX, mode=\"HF\")\n",
    "        \n",
    "        # Shapelet selection\n",
    "        self.ShapeletTransformRaw = hypergraph_shapelet_selection(trainSignalX, \n",
    "                                                                  trainY, \n",
    "                                                                  max_shapelets=self.max_shapelets,\n",
    "                                                                  per_shapelets=self.per_shapelets,\n",
    "                                                                  neighbor_k=self.neighbor_k,\n",
    "                                                                  jaccard_threshold=self.jaccard_threshold,\n",
    "                                                                  seed=self.random_state)\n",
    "        self.ShapeletTransformT0 = hypergraph_shapelet_selection(trainSignalT0X, \n",
    "                                                                 trainY, \n",
    "                                                                 max_shapelets=self.max_shapelets,\n",
    "                                                                 per_shapelets=self.per_shapelets,\n",
    "                                                                 neighbor_k=self.neighbor_k,\n",
    "                                                                 jaccard_threshold=self.jaccard_threshold,\n",
    "                                                                 seed=self.random_state)\n",
    "        self.ShapeletTransformT1 = hypergraph_shapelet_selection(trainSignalT1X, \n",
    "                                                                 trainY, \n",
    "                                                                 max_shapelets=self.max_shapelets,\n",
    "                                                                 per_shapelets=self.per_shapelets,\n",
    "                                                                 neighbor_k=self.neighbor_k,\n",
    "                                                                 jaccard_threshold=self.jaccard_threshold,\n",
    "                                                                 seed=self.random_state)\n",
    "        self.ShapeletTransformT2 = hypergraph_shapelet_selection(trainSignalT2X, \n",
    "                                                                 trainY, \n",
    "                                                                 max_shapelets=self.max_shapelets,\n",
    "                                                                 per_shapelets=self.per_shapelets,\n",
    "                                                                 neighbor_k=self.neighbor_k,\n",
    "                                                                 jaccard_threshold=self.jaccard_threshold,\n",
    "                                                                 seed=self.random_state)  \n",
    "        # Feature extraction and fusion\n",
    "        self.multiview_fusionRaw = MultiviewFusion(self.ShapeletTransformRaw, \n",
    "                                                   baseKa0=self.baseKa0, \n",
    "                                                   baseKr0=self.baseKr0,\n",
    "                                                   K_min=self.K_min, \n",
    "                                                   K_max=self.K_max,\n",
    "                                                   learn_rate=self.learn_rate, \n",
    "                                                   max_inter=self.max_inter,\n",
    "                                                   seed=self.random_state)\n",
    "        [trainX_minORI, trainX_sooORI, \n",
    "         trainX_minO_A, trainX_minO_R, trainX_minA_R,\n",
    "         trainX_sooO_A, trainX_sooO_R, trainX_sooA_R] = self.multiview_fusionRaw.fit_transform(trainSignalX, trainY)\n",
    "\n",
    "        self.multiview_fusionT0 = MultiviewFusion(self.ShapeletTransformT0, \n",
    "                                                  baseKa0=self.baseKa0, \n",
    "                                                  baseKr0=self.baseKr0,\n",
    "                                                  K_min=self.K_min, \n",
    "                                                  K_max=self.K_max,\n",
    "                                                  learn_rate=self.learn_rate, \n",
    "                                                  max_inter=self.max_inter,\n",
    "                                                  seed=self.random_state)\n",
    "        [trainT0X_minORI, trainT0X_sooORI, \n",
    "         trainT0X_minO_A, trainT0X_minO_R, trainT0X_minA_R,\n",
    "         trainT0X_sooO_A, trainT0X_sooO_R, trainT0X_sooA_R] = self.multiview_fusionT0.fit_transform(trainSignalT0X, trainY)     \n",
    "\n",
    "        self.multiview_fusionT1 = MultiviewFusion(self.ShapeletTransformT1, \n",
    "                                                  baseKa0=self.baseKa0, \n",
    "                                                  baseKr0=self.baseKr0,\n",
    "                                                  K_min=self.K_min, \n",
    "                                                  K_max=self.K_max,\n",
    "                                                  learn_rate=self.learn_rate, \n",
    "                                                  max_inter=self.max_inter,\n",
    "                                                  seed=self.random_state)\n",
    "        [trainT1X_minORI, trainT1X_sooORI, \n",
    "         trainT1X_minO_A, trainT1X_minO_R, trainT1X_minA_R,\n",
    "         trainT1X_sooO_A, trainT1X_sooO_R, trainT1X_sooA_R] = self.multiview_fusionT1.fit_transform(trainSignalT1X, trainY)         \n",
    "\n",
    "        self.multiview_fusionT2 = MultiviewFusion(self.ShapeletTransformT2, \n",
    "                                                  baseKa0=self.baseKa0, \n",
    "                                                  baseKr0=self.baseKr0,\n",
    "                                                  K_min=self.K_min, \n",
    "                                                  K_max=self.K_max,\n",
    "                                                  learn_rate=self.learn_rate, \n",
    "                                                  max_inter=self.max_inter,\n",
    "                                                  seed=self.random_state)\n",
    "        [trainT2X_minORI, trainT2X_sooORI, \n",
    "         trainT2X_minO_A, trainT2X_minO_R, trainT2X_minA_R,\n",
    "         trainT2X_sooO_A, trainT2X_sooO_R, trainT2X_sooA_R] = self.multiview_fusionT2.fit_transform(trainSignalT2X, trainY)        \n",
    "        \n",
    "        # Classifiers training\n",
    "        # View 0\n",
    "        self.clf_V0 = RidgeClassifierCV(np.logspace(-4, 4, 20))\n",
    "        self.clf_V0.fit(np.hstack((trainX_minORI, trainX_sooORI,\n",
    "                                   trainT0X_minORI, trainT0X_sooORI,\n",
    "                                   trainT1X_minORI, trainT1X_sooORI,\n",
    "                                   trainT2X_minORI, trainT2X_sooORI)), trainY)\n",
    "        # View 1\n",
    "        self.clf_V1 = RidgeClassifierCV(np.logspace(-4, 4, 20)) \n",
    "        self.clf_V1.fit(np.hstack((trainX_minORI, trainX_sooORI,   trainX_minO_A, trainX_sooO_A,\n",
    "                                   trainT0X_minORI, trainT0X_sooORI,   trainT0X_minO_A, trainT0X_sooO_A,\n",
    "                                   trainT1X_minORI, trainT1X_sooORI,   trainT1X_minO_A, trainT1X_sooO_A,\n",
    "                                   trainT2X_minORI, trainT2X_sooORI,   trainT2X_minO_A, trainT2X_sooO_A)), trainY)\n",
    "        # View 2\n",
    "        self.clf_V2 = RidgeClassifierCV(np.logspace(-4, 4, 20))  \n",
    "        self.clf_V2.fit(np.hstack((trainX_minORI, trainX_sooORI,   trainX_minO_R, trainX_sooO_R,\n",
    "                                   trainT0X_minORI, trainT0X_sooORI,   trainT0X_minO_R, trainT0X_sooO_R,\n",
    "                                   trainT1X_minORI, trainT1X_sooORI,   trainT1X_minO_R, trainT1X_sooO_R,\n",
    "                                   trainT2X_minORI, trainT2X_sooORI,   trainT2X_minO_R, trainT2X_sooO_R)), trainY)\n",
    "        # View 3\n",
    "        self.clf_V3 = RidgeClassifierCV(np.logspace(-4, 4, 20)) \n",
    "        self.clf_V3.fit(np.hstack((trainX_minORI, trainX_sooORI,   trainX_minA_R, trainX_sooA_R,\n",
    "                                   trainT0X_minORI, trainT0X_sooORI,   trainT0X_minA_R, trainT0X_sooA_R,\n",
    "                                   trainT1X_minORI, trainT1X_sooORI,   trainT1X_minA_R, trainT1X_sooA_R,\n",
    "                                   trainT2X_minORI, trainT2X_sooORI,   trainT2X_minA_R, trainT2X_sooA_R)), trainY)\n",
    "        \n",
    "        \n",
    "    def predict(self, testSignalX):\n",
    "        \"\"\"Predict class values of n instances in testSignalX.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        testSignalX : 3D np.ndarray of shape = [n_cases, n_channels, n_timepoints]\n",
    "            The data to make predictions for testSignalX.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        y : array-like, shape = [n_cases]\n",
    "            Predicted class labels.\n",
    "        \"\"\"\n",
    "        # series transform\n",
    "        testSignalT0X = series_transform(testSignalX, mode=\"F\")\n",
    "        testSignalT1X = series_transform(testSignalX, mode=\"H\")\n",
    "        testSignalT2X = series_transform(testSignalX, mode=\"HF\")\n",
    "        \n",
    "        # Feature extraction and fusion\n",
    "        [testX_minORI, testX_sooORI, \n",
    "         testX_minO_A, testX_minO_R, testX_minA_R, \n",
    "         testX_sooO_A, testX_sooO_R, testX_sooA_R] = self.multiview_fusionRaw.transform(testSignalX)\n",
    "        \n",
    "        [testT0X_minORI, testT0X_sooORI, \n",
    "         testT0X_minO_A, testT0X_minO_R, testT0X_minA_R, \n",
    "         testT0X_sooO_A, testT0X_sooO_R, testT0X_sooA_R] = self.multiview_fusionT0.transform(testSignalT0X)  \n",
    "        \n",
    "        [testT1X_minORI, testT1X_sooORI, \n",
    "         testT1X_minO_A, testT1X_minO_R, testT1X_minA_R, \n",
    "         testT1X_sooO_A, testT1X_sooO_R, testT1X_sooA_R] = self.multiview_fusionT1.transform(testSignalT1X)    \n",
    "        \n",
    "        [testT2X_minORI, testT2X_sooORI, \n",
    "         testT2X_minO_A, testT2X_minO_R, testT2X_minA_R, \n",
    "         testT2X_sooO_A, testT2X_sooO_R, testT2X_sooA_R] = self.multiview_fusionT2.transform(testSignalT2X)    \n",
    "        \n",
    "        # Ensemble classification\n",
    "        # View 0\n",
    "        testPY_V0 = self.clf_V0.predict(np.hstack((testX_minORI, testX_sooORI,\n",
    "                                                   testT0X_minORI, testT0X_sooORI,\n",
    "                                                   testT1X_minORI, testT1X_sooORI,\n",
    "                                                   testT2X_minORI, testT2X_sooORI)))\n",
    "        # View 1\n",
    "        testPY_V1 = self.clf_V1.predict(np.hstack((testX_minORI, testX_sooORI,   testX_minO_A, testX_sooO_A,\n",
    "                                                    testT0X_minORI, testT0X_sooORI,   testT0X_minO_A, testT0X_sooO_A,\n",
    "                                                    testT1X_minORI, testT1X_sooORI,   testT1X_minO_A, testT1X_sooO_A,\n",
    "                                                    testT2X_minORI, testT2X_sooORI,   testT2X_minO_A, testT2X_sooO_A)))\n",
    "        # View 2\n",
    "        testPY_V2 = self.clf_V2.predict(np.hstack((testX_minORI, testX_sooORI,   testX_minO_R, testX_sooO_R,\n",
    "                                                   testT0X_minORI, testT0X_sooORI,   testT0X_minO_R, testT0X_sooO_R,\n",
    "                                                   testT1X_minORI, testT1X_sooORI,   testT1X_minO_R, testT1X_sooO_R,\n",
    "                                                   testT2X_minORI, testT2X_sooORI,   testT2X_minO_R, testT2X_sooO_R)))\n",
    "        # View 3\n",
    "        testPY_V3 = self.clf_V3.predict(np.hstack((testX_minORI, testX_sooORI,   testX_minA_R, testX_sooA_R,\n",
    "                                                   testT0X_minORI, testT0X_sooORI,   testT0X_minA_R, testT0X_sooA_R,\n",
    "                                                   testT1X_minORI, testT1X_sooORI,   testT1X_minA_R, testT1X_sooA_R,\n",
    "                                                   testT2X_minORI, testT2X_sooORI,   testT2X_minA_R, testT2X_sooA_R)))\n",
    "        # Hard voting\n",
    "        testPY = voting(np.vstack((testPY_V0, testPY_V1, testPY_V2, testPY_V3)))\n",
    "        return testPY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed 0: 0.8857\n",
      "seed 1: 0.8743\n",
      "seed 2: 0.8800\n",
      "seed 3: 0.8743\n",
      "seed 4: 0.8800\n",
      "seed 5: 0.8686\n",
      "seed 6: 0.8800\n",
      "seed 7: 0.8743\n",
      "seed 8: 0.8800\n",
      "seed 9: 0.8686\n"
     ]
    }
   ],
   "source": [
    "# Loading time series set\n",
    "from aeon.datasets import load_arrow_head\n",
    "trainSeriesX, trainY = load_arrow_head(\"TRAIN\")\n",
    "testSeriesX, testY = load_arrow_head(\"TEST\")\n",
    "# Note that the input and output labels of our classifier are in the format of int.\n",
    "trainY, testY = trainY.astype(int), testY.astype(int)\n",
    "\n",
    "# Classification\n",
    "accList = np.zeros(10)\n",
    "for seed in range(10):\n",
    "    shapeleter = ShapeleterClassifier(random_state=seed)\n",
    "    shapeleter.fit(trainSeriesX, trainY)\n",
    "    testPY = shapeleter.predict(testSeriesX)\n",
    "    acc = np.sum(testPY==testY) / len(testY)\n",
    "    print('seed %d: %0.4f'%(seed, acc))\n",
    "    accList[seed] = acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy Results [0.8857 0.8743 0.88   0.8743 0.88   0.8686 0.88   0.8743 0.88   0.8686]\n",
      "Mean Accuracy 0.8766\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy Results\", np.round(accList, 4))\n",
    "print(\"Mean Accuracy %0.4f\"%np.mean(accList))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
